---
title: "Web Scraping"
author: "Elaina Passero"
date: "10/24/2019"
output: html_document
---

```{r setup, include=FALSE}
library(rvest)
library(tidyverse)
library(lubridate)
library(readxl)
library(pdftools)
library(readr)

```


# Simple web scraping

R can read html using either rvest, xml, or xml2 packages. Here
we are going to navigate to the Center for Snow and Avalance Studies 
[Website](https://snowstudies.org/archived-data/) and read a table in.
This table contains links to data we want to programatically download for
three sites. I don't know much about these sites, but they contain incredibly
rich snow, temperature, and precip data. 


## Reading an html 

### Extract csv links from webpage


```{r}
site_url <- 'https://snowstudies.org/archived-data/'

#Read the web url
webpage <- read_html(site_url)

#See if we can extract tables and get the data that way
tables <- webpage %>%
  html_nodes('table') %>%
  magrittr::extract2(3) %>%
  html_table(fill = TRUE)
#That didn't work, so let's try a different approach

#Extract only weblinks and then the URLs!
links <- webpage %>%
  html_nodes('a') %>%
  .[grepl('24 Hour',.)] %>%
  html_attr('href')


```

## Data Download

### Download data in a for loop

```{r}

#Grab only the name of the file by splitting out on forward slashes
splits <- str_split_fixed(links,'/',8)

#Keep only the 8th column
dataset <- splits[,8] 

#generate a file list for where the data goes
file_names <- paste0('data/',dataset)

# for(i in 1:length(file_names)){
#   download.file(links[i],destfile=file_names[i])
# }

download <- !file.exists(file_names)
evaluate <- all(download)

```

### Download data in a map

```{r}

#Map version of the same forloop (downloading 4 files)
if(evaluate = T){
  map2(links,file_names,download.file)
}


```

## Data read-in 

### Read in just the snow data as a loop

```{r}
#Pattern matching to only keep certain files
snow_files <- file_names %>%
  .[!grepl('SG_24',.)] %>%
  .[!grepl('PTSP',.)]

#empty_data <- list()

# snow_data <- for(i in 1:length(snow_files)){
#   empty_data[[i]] <- read_csv(snow_files[i]) %>%
#     select(Year,DOY,Sno_Height_M)
# }


#snow_data_full <- do.call('rbind',empty_data)

#summary(snow_data_full)
```


### Read in the data as a map function

```{r}


our_snow_reader <- function(file){
  name = str_split_fixed(file,'/',2)[,2] %>%
    gsub('_24hr.csv','',.)
  df <- read_csv(file) %>%
    select(Year,DOY,Sno_Height_M) %>%
    mutate(site = name)
}


snow_data_full <- map_dfr(snow_files,our_snow_reader)

summary(snow_data_full)
```


### Plot snow data

```{r}
snow_yearly <- snow_data_full %>%
  group_by(Year,site) %>%
  summarize(mean_height = mean(Sno_Height_M,na.rm=T))


ggplot(snow_yearly,aes(x=Year,y=mean_height,color=site)) + 
  geom_point() +
  ggthemes::theme_few() + 
  ggthemes::scale_color_few()
```


# In-Class work

## Extracting meteorological data urls

Here I want you to use the `rvest` package to get the URLs for
the `SASP forcing` and `SBSP_forcing` meteoroligical datasets.

```{r}
site_url <- 'https://snowstudies.org/archived-data/'

urls <- webpage %>%
  html_nodes('a') %>%
  .[grepl('forcing',.)] %>%
  html_attr('href')

```

## Download the meteorological data

Here I want you to use the `download_file` and `str_split_fixed` 
commands to download the data ands save it in your data folder.
You can use a for loop or a map function. 

```{r}

splits1 <- str_split_fixed(urls,'/',6)
force_dataset <- splits1[,5]
file_names1 <- paste0('data/',force_dataset)

download1 <- !file.exists(file_names1)
evaluate <- all(download1)

map2(urls,file_names1,download.file)
  
```

## Read in the data

Write a custom function to read in the data and append a site 
column to the data. 

```{r}

# Read headers from PDF
headers <- pdf_text("https://snowstudies.org/data/Serially-Complete-Metadata-text08.pdf") %>%
  readr::read_lines(.) %>%
  trimws(.) %>%
  str_split_fixed(.,"\\.",2) %>%
  .[,2] %>%
  .[1:26]

add_site <- function(path){
  file <- str_split_fixed(path,'/',2)[2] # extract file name
  name <- str_split_fixed(file,"\\-",4)[1,3] # get site name
  df <- read_delim(path, col_names = headers, delim = " ") %>%
    mutate(site = name)
}


```


Use the `map` function to read in both meteorological files.

```{r}
force_data_full <- map_dfr(file_names1,add_site)
```


